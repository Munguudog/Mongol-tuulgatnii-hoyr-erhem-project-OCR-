{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\__init__.py:238\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    236\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 238\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_labels, char_to_id, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_labels = image_labels\n",
    "        self.char_to_id = char_to_id\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Normalize the image\n",
    "        img = img / 255.0\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Convert to tensor and add channel dimension\n",
    "        img = torch.FloatTensor(img).unsqueeze(0)\n",
    "        \n",
    "        # Get label\n",
    "        label = torch.LongTensor(self.image_labels[idx])\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Collate function to handle variable length sequences\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Pad images to same height and width if necessary\n",
    "    max_h = max([img.shape[1] for img in images])\n",
    "    max_w = max([img.shape[2] for img in images])\n",
    "    \n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        pad_h = max_h - img.shape[1]\n",
    "        pad_w = max_w - img.shape[2]\n",
    "        padded_img = nn.functional.pad(img, (0, pad_w, 0, pad_h), \"constant\", 0)\n",
    "        padded_images.append(padded_img)\n",
    "    \n",
    "    # Stack all images into a batch\n",
    "    images = torch.stack(padded_images)\n",
    "    \n",
    "    # Store original label lengths for CTC loss\n",
    "    label_lengths = torch.LongTensor([len(label) for label in labels])\n",
    "    \n",
    "    # Pad labels for batch processing\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, labels, label_lengths\n",
    "\n",
    "# CTC Loss wrapper\n",
    "class CTCLoss(nn.Module):\n",
    "    def __init__(self, blank=0):\n",
    "        super(CTCLoss, self).__init__()\n",
    "        self.ctc = nn.CTCLoss(blank=blank, reduction='mean', zero_infinity=True)\n",
    "    \n",
    "    def forward(self, log_probs, targets, input_lengths, target_lengths):\n",
    "        return self.ctc(log_probs, targets, input_lengths, target_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, vocab_size, img_channels=1, hidden_size=128, num_lstm_layers=2):  # Reduced from 256 to 128\n",
    "        super(OCRModel, self).__init__()\n",
    "\n",
    "        # CNN Feature Extractor (Reduce Width Pooling)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  # (B, 64, 50, 250)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  # (B, 128, 25, 125)\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # (B, 256, 12, 125)\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),  # Reduced from 512 to 384\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))   # (B, 384, 6, 125)\n",
    "        )\n",
    "\n",
    "        # Add a 1x1 convolution to reduce channels before LSTM\n",
    "        self.channel_reducer = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=1),  # Reduce channels with 1x1 conv\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Compute LSTM input size dynamically\n",
    "        sample_input = torch.randn(1, img_channels, 100, 500)\n",
    "        with torch.no_grad():\n",
    "            cnn_output = self.cnn(sample_input)\n",
    "            reduced_output = self.channel_reducer(cnn_output)\n",
    "            _, C, H, W = reduced_output.shape\n",
    "            self.lstm_input_size = C * H  # Should be smaller than before\n",
    "            self.width_after_cnn = W\n",
    "\n",
    "        # LSTM Sequence Model with smaller hidden size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=0.2  # Add dropout directly in LSTM\n",
    "        )\n",
    "        \n",
    "        # Extra dropout between LSTM and FC\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Fully connected layer (Bidirectional LSTM)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN forward\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Reduce channels\n",
    "        features = self.channel_reducer(features)\n",
    "\n",
    "        # Reshape for LSTM (treat width as time-steps)\n",
    "        b, c, h, w = features.size()\n",
    "        features = features.permute(0, 3, 1, 2).contiguous().view(b, w, -1)\n",
    "\n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Fully connected\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        # Apply log_softmax for CTC loss stability\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Initialize weights properly\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(param, gain=0.8)  # Reduced gain factor for stability\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)  # Initialize biases to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character mappings loaded successfully!\n",
      "Loaded char_to_id: {' ': 1, '?': 2, '᠂': 3, '᠃': 4, '᠋': 5, '᠌': 6, '᠍': 7, '\\u180e': 8, 'ᠠ': 9, 'ᠡ': 10, 'ᠢ': 11, 'ᠣ': 12, 'ᠤ': 13, 'ᠥ': 14, 'ᠦ': 15, 'ᠧ': 16, 'ᠨ': 17, 'ᠩ': 18, 'ᠪ': 19, 'ᠬ': 20, 'ᠭ': 21, 'ᠮ': 22, 'ᠯ': 23, 'ᠰ': 24, 'ᠱ': 25, 'ᠲ': 26, 'ᠳ': 27, 'ᠴ': 28, 'ᠵ': 29, 'ᠶ': 30, 'ᠷ': 31, 'ᠹ': 32, '\\u202f': 33, '︖': 34, '？': 35}\n",
      "Loaded id_to_char: {1: ' ', 2: '?', 3: '᠂', 4: '᠃', 5: '᠋', 6: '᠌', 7: '᠍', 8: '\\u180e', 9: 'ᠠ', 10: 'ᠡ', 11: 'ᠢ', 12: 'ᠣ', 13: 'ᠤ', 14: 'ᠥ', 15: 'ᠦ', 16: 'ᠧ', 17: 'ᠨ', 18: 'ᠩ', 19: 'ᠪ', 20: 'ᠬ', 21: 'ᠭ', 22: 'ᠮ', 23: 'ᠯ', 24: 'ᠰ', 25: 'ᠱ', 26: 'ᠲ', 27: 'ᠳ', 28: 'ᠴ', 29: 'ᠵ', 30: 'ᠶ', 31: 'ᠷ', 32: 'ᠹ', 33: '\\u202f', 34: '︖', 35: '？'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/mini_qa_images/mini_qa.csv\")\n",
    "with open(\"../dataset/mini_qa_images/char_mappings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_mapping = json.load(f)\n",
    "\n",
    "# Convert keys back to int for id_to_char (JSON keys are saved as strings)\n",
    "char_to_id = loaded_mapping[\"char_to_id\"]\n",
    "id_to_char = {int(k): v for k, v in loaded_mapping[\"id_to_char\"].items()}\n",
    "\n",
    "print(\"Character mappings loaded successfully!\")\n",
    "print(\"Loaded char_to_id:\", char_to_id)\n",
    "print(\"Loaded id_to_char:\", id_to_char)\n",
    "\n",
    "def text_to_ids(text):\n",
    "    return [char_to_id.get(char, 0) for char in text]  # Defaulting to 0 for unknown characters\n",
    "def ids_to_text(ids):\n",
    "    return ''.join([id_to_char.get(id, '?') for id in ids])  # Use '?' for unknown IDs\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    # if(index%20==0):\n",
    "    #     print(f\"{index} out of 100\")\n",
    "    text = row['question']\n",
    "    path = f\"../dataset/mini_qa_images/question/{index}.png\"\n",
    "    image_paths.append(path)\n",
    "    image_labels.append(text_to_ids(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for images, labels, label_lengths in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate input lengths for CTC loss\n",
    "        batch_size, time_steps, _ = outputs.size()\n",
    "        input_lengths = torch.full((batch_size,), time_steps, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    return running_loss / batch_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, id_to_char):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_lengths in tqdm(dataloader, desc=\"Validating\"):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate input lengths for CTC loss\n",
    "            batch_size, time_steps, _ = outputs.size()\n",
    "            input_lengths = torch.full((batch_size,), time_steps, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = greedy_decode(outputs, id_to_char)\n",
    "            \n",
    "            # Convert labels to strings\n",
    "            ground_truths = []\n",
    "            for i in range(len(labels)):\n",
    "                gt = ''.join([id_to_char[id.item()] for id in labels[i][:label_lengths[i]]])\n",
    "                ground_truths.append(gt)\n",
    "            \n",
    "            # Store for computing metrics\n",
    "            all_predictions.extend(predictions)\n",
    "            all_ground_truths.extend(ground_truths)\n",
    "            \n",
    "            # Character accuracy\n",
    "            for pred, gt in zip(predictions, ground_truths):\n",
    "                correct_chars += sum([p == g for p, g in zip(pred[:min(len(pred), len(gt))], gt[:min(len(pred), len(gt))])])\n",
    "                total_chars += len(gt)\n",
    "    \n",
    "    # Compute metrics\n",
    "    val_loss = running_loss / batch_count\n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    \n",
    "    # Compute sequence accuracy (exact matches)\n",
    "    sequence_matches = sum([p == g for p, g in zip(all_predictions, all_ground_truths)])\n",
    "    sequence_accuracy = sequence_matches / len(all_predictions) if all_predictions else 0\n",
    "    \n",
    "    return val_loss, char_accuracy, sequence_accuracy, all_predictions, all_ground_truths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(output, id_to_char):\n",
    "    # Get most likely class\n",
    "    predictions = torch.argmax(output, dim=2).detach().cpu().numpy()\n",
    "    \n",
    "    # Convert to characters\n",
    "    decoded = []\n",
    "    for pred in predictions:\n",
    "        # Remove repeating characters\n",
    "        collapsed = []\n",
    "        prev = -1\n",
    "        for p in pred:\n",
    "            if p != prev:  # CTC decoding - collapse repeats\n",
    "                collapsed.append(p)\n",
    "            prev = p\n",
    "        \n",
    "        # Remove blanks (assuming blank is 0)\n",
    "        text = ''.join([id_to_char[p] for p in collapsed if p != 0])\n",
    "        decoded.append(text)\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "# Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(dataloader, model, id_to_char, device, num_samples=5):\n",
    "    model.eval()\n",
    "    images, labels, label_lengths = next(iter(dataloader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = greedy_decode(outputs, id_to_char)\n",
    "    \n",
    "    # Only show up to num_samples\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        plt.imshow(images[i, 0].cpu().numpy(), cmap='gray')\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt = ''.join([id_to_char[id.item()] for id in labels[i][:label_lengths[i]]])\n",
    "        \n",
    "        plt.title(f'Prediction: {predictions[i]}\\nGround Truth: {gt}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, id_to_char, checkpoint_dir='checkpoints'):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'char_accuracy': [],\n",
    "        'sequence_accuracy': []\n",
    "    }\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, char_accuracy, sequence_accuracy, predictions, ground_truths = validate(model, val_loader, criterion, device, id_to_char)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['char_accuracy'].append(char_accuracy)\n",
    "        history['sequence_accuracy'].append(sequence_accuracy)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Character Accuracy: {char_accuracy:.4f} | Sequence Accuracy: {sequence_accuracy:.4f}\")\n",
    "        \n",
    "        # Sample predictions\n",
    "        print(\"\\nSample Predictions:\")\n",
    "        for i in range(min(5, len(predictions))):\n",
    "            print(f\"Prediction: {predictions[i]}\")\n",
    "            print(f\"Ground Truth: {ground_truths[i]}\")\n",
    "            print()\n",
    "        \n",
    "        # Save checkpoint if best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_path = f\"{checkpoint_dir}/best_model_{timestamp}_ep{epoch+1}_loss{val_loss:.4f}_acc{char_accuracy:.4f}.pt\"\n",
    "            \n",
    "            print(f\"Saving best model checkpoint to {checkpoint_path}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'char_accuracy': char_accuracy,\n",
    "                'sequence_accuracy': sequence_accuracy,\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "        # Save regular checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = f\"{checkpoint_dir}/model_epoch{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'char_accuracy': char_accuracy,\n",
    "                'sequence_accuracy': sequence_accuracy,\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    # Training complete\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history['char_accuracy'], label='Character Accuracy')\n",
    "    plt.plot(history['sequence_accuracy'], label='Sequence Accuracy')\n",
    "    plt.title('Accuracy Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae21219ebe348159da4137914c85d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 22940) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 50\u001b[0m\n\u001b[0;32m     45\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[0;32m     46\u001b[0m     optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_char\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Plot history\u001b[39;00m\n\u001b[0;32m     56\u001b[0m plot_history(history)\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, id_to_char, checkpoint_dir)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      4\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels, label_lengths \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 22940) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "\n",
    "# Assuming you have these defined:\n",
    "# image_paths, image_labels, char_to_id, id_to_char\n",
    "\n",
    "# Split data into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, image_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OCRDataset(train_paths, train_labels, char_to_id)\n",
    "val_dataset = OCRDataset(val_paths, val_labels, char_to_id)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(char_to_id)\n",
    "model = OCRModel(vocab_size=vocab_size)\n",
    "model.apply(init_weights)  # Initialize weights\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = CTCLoss(blank=0)  # Assuming 0 is blank\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model, history = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "    num_epochs, device, id_to_char\n",
    ")\n",
    "\n",
    "# Plot history\n",
    "plot_history(history)\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(val_loader, model, id_to_char, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mnocr)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
