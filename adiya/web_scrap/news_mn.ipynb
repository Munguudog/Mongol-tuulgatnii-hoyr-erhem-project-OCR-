{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63066206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://news.mn/mng/category/niigem/page/\"\n",
    "NUM_PAGES = 168\n",
    "DATA_FILE = \"scraped_data/news_mn.json\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "# Load existing data if present\n",
    "if os.path.exists(DATA_FILE):\n",
    "    with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        OUTPUT = json.load(f)\n",
    "else:\n",
    "    OUTPUT = []\n",
    "\n",
    "scraped_links = set(item[\"link\"] for item in OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376da86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PAGES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6012a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing list page 1...\n",
      "Processing list page 2...\n",
      "Processing list page 3...\n",
      "Processing list page 4...\n",
      "Processing list page 5...\n",
      "Processing list page 6...\n",
      "Processing list page 7...\n",
      "Processing list page 8...\n",
      "Processing list page 9...\n",
      "Processing list page 10...\n",
      "Processing list page 11...\n",
      "Processing list page 12...\n",
      "Processing list page 13...\n",
      "Processing list page 14...\n",
      "Processing list page 15...\n",
      "Processing list page 16...\n",
      "Processing list page 17...\n",
      "Processing list page 18...\n",
      "Processing list page 19...\n",
      "Processing list page 20...\n",
      "Processing list page 21...\n",
      "Processing list page 22...\n",
      "Processing list page 23...\n",
      "Processing list page 24...\n",
      "Processing list page 25...\n",
      "Processing list page 26...\n",
      "Processing list page 27...\n",
      "Processing list page 28...\n",
      "Processing list page 29...\n",
      "Processing list page 30...\n",
      "Processing list page 31...\n",
      "Processing list page 32...\n",
      "Processing list page 33...\n",
      "Processing list page 34...\n",
      "Processing list page 35...\n",
      "Processing list page 36...\n",
      "Processing list page 37...\n",
      "Processing list page 38...\n",
      "Processing list page 39...\n",
      "Processing list page 40...\n",
      "Processing list page 41...\n",
      "Processing list page 42...\n",
      "Processing list page 43...\n",
      "Processing list page 44...\n",
      "Processing list page 45...\n",
      "Processing list page 46...\n",
      "Processing list page 47...\n",
      "Processing list page 48...\n",
      "Processing list page 49...\n",
      "Processing list page 50...\n",
      "Processing list page 51...\n",
      "Processing list page 52...\n",
      "Processing list page 53...\n",
      "Processing list page 54...\n",
      "Processing list page 55...\n",
      "Processing list page 56...\n",
      "Processing list page 57...\n",
      "Processing list page 58...\n",
      "Processing list page 59...\n",
      "Processing list page 60...\n",
      "Processing list page 61...\n",
      "Processing list page 62...\n",
      "Processing list page 63...\n",
      "Processing list page 64...\n",
      "Processing list page 65...\n",
      "Processing list page 66...\n",
      "Processing list page 67...\n",
      "Processing list page 68...\n",
      "Processing list page 69...\n",
      "Processing list page 70...\n",
      "Processing list page 71...\n",
      "Processing list page 72...\n",
      "Processing list page 73...\n",
      "Processing list page 74...\n",
      "Processing list page 75...\n",
      "Processing list page 76...\n",
      "Processing list page 77...\n",
      "Processing list page 78...\n",
      "Processing list page 79...\n",
      "Processing list page 80...\n",
      "Processing list page 81...\n",
      "Processing list page 82...\n",
      "Processing list page 83...\n",
      "Processing list page 84...\n",
      "Processing list page 85...\n",
      "Processing list page 86...\n",
      "Processing list page 87...\n",
      "Processing list page 88...\n",
      "Processing list page 89...\n",
      "Processing list page 90...\n",
      "Processing list page 91...\n",
      "Processing list page 92...\n",
      "Processing list page 93...\n",
      "Processing list page 94...\n",
      "Processing list page 95...\n",
      "Processing list page 96...\n",
      "Processing list page 97...\n",
      "Processing list page 98...\n",
      "Processing list page 99...\n",
      "Processing list page 100...\n",
      "Processing list page 101...\n",
      "Processing list page 102...\n",
      "Processing list page 103...\n",
      "Processing list page 104...\n",
      "Processing list page 105...\n",
      "Processing list page 106...\n",
      "Processing list page 107...\n",
      "Processing list page 108...\n",
      "Processing list page 109...\n",
      "Processing list page 110...\n",
      "Processing list page 111...\n",
      "Processing list page 112...\n",
      "Processing list page 113...\n",
      "Processing list page 114...\n",
      "Processing list page 115...\n",
      "Processing list page 116...\n",
      "Processing list page 117...\n",
      "Processing list page 118...\n",
      "Processing list page 119...\n",
      "Processing list page 120...\n",
      "Processing list page 121...\n",
      "Processing list page 122...\n",
      "Processing list page 123...\n",
      "Processing list page 124...\n",
      "Processing list page 125...\n",
      "Processing list page 126...\n",
      "Processing list page 127...\n",
      "Processing list page 128...\n",
      "Processing list page 129...\n",
      "Processing list page 130...\n",
      "Processing list page 131...\n",
      "Processing list page 132...\n",
      "Processing list page 133...\n",
      "Processing list page 134...\n",
      "Processing list page 135...\n",
      "Processing list page 136...\n",
      "Processing list page 137...\n",
      "Processing list page 138...\n",
      "Processing list page 139...\n",
      "Processing list page 140...\n",
      "Processing list page 141...\n",
      "Processing list page 142...\n",
      "Processing list page 143...\n",
      "Processing list page 144...\n",
      "Processing list page 145...\n",
      "Processing list page 146...\n",
      "Processing list page 147...\n",
      "Processing list page 148...\n",
      "Processing list page 149...\n",
      "Processing list page 150...\n",
      "Processing list page 151...\n",
      "Processing list page 152...\n",
      "Processing list page 153...\n",
      "Processing list page 154...\n",
      "Processing list page 155...\n",
      "Processing list page 156...\n",
      "Processing list page 157...\n",
      "Processing list page 158...\n",
      "Processing list page 159...\n",
      "Processing list page 160...\n",
      "Processing list page 161...\n",
      "Processing list page 162...\n",
      "Processing list page 163...\n",
      "Processing list page 164...\n",
      "Processing list page 165...\n",
      "Processing list page 166...\n",
      "Processing list page 167...\n",
      "Processing list page 168...\n"
     ]
    }
   ],
   "source": [
    "for page in range(1, NUM_PAGES + 1):\n",
    "    print(f\"Processing list page {page}...\")\n",
    "    page_url = f\"{BASE_URL}{page}/\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        articles = soup.find_all(\"h2\", class_=\"entry-title\")\n",
    "\n",
    "        for h2 in articles:\n",
    "            a_tag = h2.find(\"a\")\n",
    "            if not a_tag:\n",
    "                continue\n",
    "\n",
    "            link = a_tag[\"href\"]\n",
    "\n",
    "            if link in scraped_links:\n",
    "                print(f\"Skipping already scraped {link}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                post_resp = requests.get(link, headers=headers)\n",
    "                post_soup = BeautifulSoup(post_resp.text, \"html.parser\")\n",
    "\n",
    "                title_tag = post_soup.find(\"h2\", class_=\"entry-title\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"No Title Found\"\n",
    "\n",
    "                content_div = post_soup.find(\"div\", class_=\"single-content entry-content uk-clearfix\")\n",
    "                if content_div:\n",
    "                    paragraphs = [p.get_text(strip=True) for p in content_div.find_all(\"p\")]\n",
    "                else:\n",
    "                    paragraphs = []\n",
    "\n",
    "                post_data = {\n",
    "                    \"link\": link,\n",
    "                    \"title\": title,\n",
    "                    \"data\": paragraphs\n",
    "                }\n",
    "\n",
    "                OUTPUT.append(post_data)\n",
    "                scraped_links.add(link)\n",
    "\n",
    "                # print(f\"Scraped: {title}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to process article {link}: {e}\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load page {page}: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a05afb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done scraping news.mn!\n"
     ]
    }
   ],
   "source": [
    "# Save all data after scraping\n",
    "with open(DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(OUTPUT, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Done scraping news.mn!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AND",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
