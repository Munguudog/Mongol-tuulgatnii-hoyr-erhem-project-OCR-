{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dataset handle: uljibuhborjigin/mongol-ocr-zcode/mn_ocr_synthetic/mn_ocr_synthetic/this_uni_to_z_outputs.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muljibuhborjigin/mongol-ocr-zcode/mn_ocr_synthetic/mn_ocr_synthetic/this_uni_to_z_outputs.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\kagglehub\\datasets.py:33\u001b[0m, in \u001b[0;36mdataset_download\u001b[1;34m(handle, path, force_download)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdataset_download\u001b[39m(handle: \u001b[38;5;28mstr\u001b[39m, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download dataset files\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        handle: (string) the dataset handle\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m        A string requesting the path to the requested dataset files.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mparse_dataset_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m     35\u001b[0m     path, _ \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mdataset_resolver(h, path, force_download\u001b[38;5;241m=\u001b[39mforce_download)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\kagglehub\\handle.py:161\u001b[0m, in \u001b[0;36mparse_dataset_handle\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetHandle(\n\u001b[0;32m    155\u001b[0m         owner\u001b[38;5;241m=\u001b[39mparts[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    156\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mparts[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    157\u001b[0m         version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    160\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dataset handle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhandle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dataset handle: uljibuhborjigin/mongol-ocr-zcode/mn_ocr_synthetic/mn_ocr_synthetic/this_uni_to_z_outputs.csv"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uljibuhborjigin/mongol-ocr-zcode/mn_ocr_synthetic/mn_ocr_synthetic/this_uni_to_z_outputs.csv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load Kaggle credentials\n",
    "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\")) as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Construct URL and headers\n",
    "dataset_owner = \"uljibuhborjigin\"\n",
    "dataset_name = \"mongol-ocr-zcode\"\n",
    "file_path = \"mn_ocr_synthetic/mn_ocr_synthetic/this_uni_to_z_outputs.csv\"\n",
    "output_path = \"C:/MNOCR/dataset/ocr_dataset/this_uni_to_z_outputs.csv\"\n",
    "\n",
    "url = f\"https://www.kaggle.com/api/v1/datasets/download/{dataset_owner}/{dataset_name}/{file_path}\"\n",
    "headers = {\"Authorization\": f\"Bearer {kaggle_creds['key']}\"}\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, headers=headers, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded to: {output_path}\")\n",
    "else:\n",
    "    print(\"Failed to download:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, max_label_length=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of paths to images\n",
    "            labels: List of lists of character IDs (already converted to IDs)\n",
    "            transform: Optional transform to be applied on images\n",
    "            max_label_length: Maximum length for label padding\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_label_length = max_label_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "            \n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Get label\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            # Return a placeholder in case of error\n",
    "            if self.transform:\n",
    "                placeholder = torch.zeros((1, 100, 400))  # Adjust size as needed\n",
    "            else:\n",
    "                placeholder = Image.new('L', (400, 100))\n",
    "                placeholder = self.transform(placeholder) if self.transform else placeholder\n",
    "            return placeholder, [1]  # Return a simple label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_classes, hidden_size=64, num_layers=1):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # CNN Backbone - SimpleNet to avoid complexity\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Pool only height\n",
    "            \n",
    "            # Layer 4 \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))  # Pool only height\n",
    "        )\n",
    "        \n",
    "        # Calculate width after CNN using a sample input\n",
    "        self.width_test = torch.randn(1, input_channels, 32, 200)  # Adjust sample size as needed\n",
    "        cnn_output = self.cnn(self.width_test)\n",
    "        _, channels, height, width = cnn_output.shape\n",
    "        self.output_height = height\n",
    "        self.width_after_cnn = width\n",
    "        self.rnn_input_size = channels * height\n",
    "        \n",
    "        # RNN Layer - Simple and stable\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_classes)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # X shape: (batch, channels, height, width)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply CNN features extraction\n",
    "        conv = self.cnn(x)\n",
    "        \n",
    "        # Prepare for RNN (batch, width, channels*height)\n",
    "        # Permute from (batch, channels, height, width) to (batch, width, channels, height)\n",
    "        conv = conv.permute(0, 3, 1, 2)\n",
    "        # Reshape to (batch, width, channels*height)\n",
    "        conv = conv.reshape(batch_size, conv.size(1), -1)\n",
    "        \n",
    "        # Apply RNN\n",
    "        rnn_output, _ = self.rnn(conv)\n",
    "        \n",
    "        # Dropout\n",
    "        rnn_output = self.dropout(rnn_output)\n",
    "        \n",
    "        # Linear layer for output probabilities\n",
    "        output = self.linear(rnn_output)\n",
    "        \n",
    "        # Apply log_softmax for CTC Loss\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ocr_model(image_paths, labels, char_to_id, id_to_char, batch_size=8, num_epochs=10, \n",
    "                    learning_rate=0.001, save_dir='checkpoints'):\n",
    "    \"\"\"\n",
    "    Train OCR model end-to-end\n",
    "    Args:\n",
    "        image_paths: List of image paths\n",
    "        labels: List of labels (each label is a list of character IDs)\n",
    "        char_to_id: Dictionary mapping characters to IDs\n",
    "        id_to_char: Dictionary mapping IDs to characters\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        save_dir: Directory to save model checkpoints\n",
    "    \"\"\"\n",
    "    # Make sure save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 200)),  # Consistent but not too large size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = OCRDataset(image_paths, labels, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn  # Custom collate function defined below\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    vocab_size = len(char_to_id) + 1  # +1 for blank token in CTC\n",
    "    model = CRNN(\n",
    "        input_channels=1,  # Grayscale images\n",
    "        output_classes=vocab_size,\n",
    "        hidden_size=64,  # Small hidden size for stability\n",
    "        num_layers=1  # Single layer for stability\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model information\n",
    "    print(f\"Model width after CNN: {model.width_after_cnn}\")\n",
    "    print(f\"Model RNN input size: {model.rnn_input_size}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total model parameters: {total_params:,}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=2, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        valid_batches = 0\n",
    "        \n",
    "        for i, (images, targets, target_lengths) in enumerate(dataloader):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                target_lengths = target_lengths.to(device)\n",
    "                \n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Check for NaNs in output\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"NaN detected in outputs at batch {i}, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate output lengths (assuming all sequences have same width)\n",
    "                batch_size = images.size(0)\n",
    "                output_lengths = torch.full(\n",
    "                    (batch_size,), model.width_after_cnn, dtype=torch.long\n",
    "                ).to(device)\n",
    "                \n",
    "                # Transpose output for CTC loss: (batch, time, classes) -> (time, batch, classes)\n",
    "                outputs = outputs.permute(1, 0, 2)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, targets, output_lengths, target_lengths)\n",
    "                \n",
    "                # Check for NaN loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"NaN loss at batch {i}, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "                \n",
    "                # Check for NaN gradients\n",
    "                has_nan_grad = False\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                        has_nan_grad = True\n",
    "                        break\n",
    "                        \n",
    "                if has_nan_grad:\n",
    "                    print(f\"NaN gradients detected at batch {i}, skipping update\")\n",
    "                    continue\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                batch_loss = loss.item()\n",
    "                epoch_loss += batch_loss\n",
    "                valid_batches += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Print progress\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], \"\n",
    "                          f\"Loss: {batch_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # End of epoch\n",
    "        if valid_batches > 0:\n",
    "            avg_epoch_loss = epoch_loss / valid_batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_epoch_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_epoch_loss)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if avg_epoch_loss < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss,\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
    "                print(f\"Saved best model checkpoint with loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Always save the last model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss / max(1, valid_batches),\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(save_dir, f'model_epoch_{epoch+1}.pt'))\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Custom collate function to handle variable length labels\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for variable length OCR labels\n",
    "    \"\"\"\n",
    "    # Separate images and labels\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Stack all images into a batch\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    # Get lengths of each label\n",
    "    lengths = torch.tensor([len(label) for label in labels])\n",
    "    \n",
    "    # Flatten and concatenate all labels\n",
    "    labels_concat = torch.cat([torch.tensor(label) for label in labels])\n",
    "    \n",
    "    return images, labels_concat, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions, id_to_char):\n",
    "    \"\"\"\n",
    "    Decode model predictions to text\n",
    "    Args:\n",
    "        predictions: Model output after softmax\n",
    "        id_to_char: Mapping from ID to character\n",
    "    Returns:\n",
    "        Decoded text\n",
    "    \"\"\"\n",
    "    # Get best prediction (greedy decoding)\n",
    "    _, max_indices = torch.max(predictions, dim=2)\n",
    "    \n",
    "    # Convert to numpy for easier processing\n",
    "    max_indices = max_indices.cpu().numpy()\n",
    "    \n",
    "    # Decode batch\n",
    "    decoded_texts = []\n",
    "    \n",
    "    for indices in max_indices:\n",
    "        # Create text (CTC blank removal and repeated character removal)\n",
    "        text = \"\"\n",
    "        prev_idx = -1\n",
    "        \n",
    "        for idx in indices:\n",
    "            # Skip blank token (usually 0) and repeated characters\n",
    "            if idx != 0 and idx != prev_idx:\n",
    "                text += id_to_char.get(idx, \"\")  # Get character from ID\n",
    "            prev_idx = idx\n",
    "        \n",
    "        decoded_texts.append(text)\n",
    "    \n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character mappings loaded successfully!\n",
      "Loaded char_to_id: {' ': 1, '?': 2, '᠂': 3, '᠃': 4, '᠋': 5, '᠌': 6, '᠍': 7, '\\u180e': 8, 'ᠠ': 9, 'ᠡ': 10, 'ᠢ': 11, 'ᠣ': 12, 'ᠤ': 13, 'ᠥ': 14, 'ᠦ': 15, 'ᠧ': 16, 'ᠨ': 17, 'ᠩ': 18, 'ᠪ': 19, 'ᠬ': 20, 'ᠭ': 21, 'ᠮ': 22, 'ᠯ': 23, 'ᠰ': 24, 'ᠱ': 25, 'ᠲ': 26, 'ᠳ': 27, 'ᠴ': 28, 'ᠵ': 29, 'ᠶ': 30, 'ᠷ': 31, 'ᠹ': 32, '\\u202f': 33, '︖': 34, '？': 35}\n",
      "Loaded id_to_char: {1: ' ', 2: '?', 3: '᠂', 4: '᠃', 5: '᠋', 6: '᠌', 7: '᠍', 8: '\\u180e', 9: 'ᠠ', 10: 'ᠡ', 11: 'ᠢ', 12: 'ᠣ', 13: 'ᠤ', 14: 'ᠥ', 15: 'ᠦ', 16: 'ᠧ', 17: 'ᠨ', 18: 'ᠩ', 19: 'ᠪ', 20: 'ᠬ', 21: 'ᠭ', 22: 'ᠮ', 23: 'ᠯ', 24: 'ᠰ', 25: 'ᠱ', 26: 'ᠲ', 27: 'ᠳ', 28: 'ᠴ', 29: 'ᠵ', 30: 'ᠶ', 31: 'ᠷ', 32: 'ᠹ', 33: '\\u202f', 34: '︖', 35: '？'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/mini_qa_images/mini_qa.csv\")\n",
    "with open(\"../dataset/mini_qa_images/char_mappings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_mapping = json.load(f)\n",
    "\n",
    "# Convert keys back to int for id_to_char (JSON keys are saved as strings)\n",
    "char_to_id = loaded_mapping[\"char_to_id\"]\n",
    "id_to_char = {int(k): v for k, v in loaded_mapping[\"id_to_char\"].items()}\n",
    "\n",
    "print(\"Character mappings loaded successfully!\")\n",
    "print(\"Loaded char_to_id:\", char_to_id)\n",
    "print(\"Loaded id_to_char:\", id_to_char)\n",
    "\n",
    "def text_to_ids(text):\n",
    "    return [char_to_id.get(char, 0) for char in text]  # Defaulting to 0 for unknown characters\n",
    "def ids_to_text(ids):\n",
    "    return ''.join([id_to_char.get(id, '?') for id in ids])  # Use '?' for unknown IDs\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    # if(index%20==0):\n",
    "    #     print(f\"{index} out of 100\")\n",
    "    text = row['question']\n",
    "    path = f\"../dataset/mini_qa_images/question/{index}.png\"\n",
    "    image_paths.append(path)\n",
    "    image_labels.append(text_to_ids(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model width after CNN: 50\n",
      "Model RNN input size: 256\n",
      "Total model parameters: 369,252\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2416, 17908) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_ocr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_to_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_char\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 70\u001b[0m, in \u001b[0;36mtrain_ocr_model\u001b[1;34m(image_paths, labels, char_to_id, id_to_char, batch_size, num_epochs, learning_rate, save_dir)\u001b[0m\n\u001b[0;32m     67\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     68\u001b[0m valid_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, targets, target_lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m     73\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\home\\anaconda3\\envs\\mnocr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2416, 17908) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_ocr_model(image_paths, image_labels, char_to_id, id_to_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mnocr)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
